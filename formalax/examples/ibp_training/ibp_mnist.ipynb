{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training robust deep learning model using IBP (MNIST)\n",
    "\n",
    "This notebook uses IBP to train robust models, following the approach of [Gowal et al., 2019](https://arxiv.org/pdf/1810.12715) with the improvements proposed by [Shi et al., 2021](https://proceedings.neurips.cc/paper/2021/hash/988f9153ac4fd966ea302dd9ab9bae15-Abstract.html)\n",
    "Fundamentally, the code is based on [this script](https://github.com/google-deepmind/interval-bound-propagation/blob/master/examples/train.py) from [Gowal et al., 2019](https://arxiv.org/pdf/1810.12715) and is translated into the JAX framework.\n",
    "\n",
    "1) Data loading and train/test split. \n",
    "2) IBP initialization (Shi et al., 2021)\n",
    "3) Model definition\n",
    "4) Loss function\n",
    "5) Evaluation function\n",
    "6) Training & Evaluatio\n",
    "\n",
    "#### Run using Papermill\n",
    "```bash\n",
    "papermill ibp_mnist.ipynb -f PARAMETERS_FILE.yaml OUTPUT_FILE.ipynb\n",
    "```\n",
    "For example:\n",
    "```bash\n",
    "papermill ibp_mnist.ipynb -p dataset FashionMNIST ibp_fashion_mnist_out.ipynb\n",
    "```\n",
    "or\n",
    "```bash\n",
    "papermill ibp_mnist.ipynb -f emnist_params.yaml ibp_emnist_out.ipynb\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import math\n",
    "import pickle\n",
    "from functools import partial\n",
    "from typing import Any, Sequence\n",
    "\n",
    "import flax.training.train_state\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "from flax import linen as nn\n",
    "from jax.nn.initializers import Initializer\n",
    "from torchvision.datasets import EMNIST, MNIST, FashionMNIST\n",
    "\n",
    "from formalax import Box, ibp"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Parameter Cell*: Parameterize and execute this notebook using [papermill](https://papermill.readthedocs.io/en/latest/).\n",
    "- Supported datasets: MNIST, EMNIST and FashionMNIST.\n",
    "- `eps`: Robustness perturbation radius.\n",
    "- `num_epochs`: A 3-tuple of epoch numbers: `(prepare, warmup, main)`.\n",
    "  The training process first runs `prepare` epochs without the robustness loss, then \n",
    "  linearly increaes the perturbation radius `eps` from zero to the final value in \n",
    "  the `warmup` epochs and finally trains with the full `eps` for `main` epochs.\n",
    "- `model_arch`: Architecture of the neural network to train.\n",
    "  The available layers are:\n",
    "    - `(\"conv\", filters, kernel_size)`: A convolutional layer with are square kernel, followed by a batch normalization layer.\n",
    "    - `(\"relu\",)`: A ReLU layer.\n",
    "    - `(\"avg_pool\", kernel_size, stride)`: An average pooling layer.\n",
    "    - `(\"dense\", features)`: A dense layer, followed by a batch normalization layer.\n",
    "  Following\n",
    "  Flattening layers are added automatically.\n",
    "  Implicitly, the final layer is always a dense layer with the number of classes as features.\n",
    "  You do not have to include this layer.\n",
    "\n",
    "For EMNIST, we use the `bymerge` split."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "source": [
    "dataset: str = \"MNIST\"\n",
    "eps: float = 0.1\n",
    "learning_rate: float = 0.001\n",
    "robustness_loss_weight: float = 0.5\n",
    "weight_decay: float = 1e-5\n",
    "batch_size: int = 128\n",
    "num_epochs: tuple[int, int, int] = (0, 20, 50)\n",
    "seed: int = 0\n",
    "\n",
    "model_arch: list[tuple[str, ...]] = [\n",
    "    (\"conv\", 32, 3),\n",
    "    (\"relu\",),\n",
    "    (\"avg_pool\", 2, 2),\n",
    "    (\"conv\", 64, 3),\n",
    "    (\"relu\",),\n",
    "    (\"avg_pool\", 2, 2),\n",
    "    (\"dense\", 256),\n",
    "    (\"relu\",),\n",
    "]\n",
    "\n",
    "data_dir: str = \"../.datasets\"\n",
    "eval_batch_size: str = 2**11"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Data\n",
    "We use PyTorch to load the datasets.\n",
    "Normalize the pixel values to [0, 1]."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "match dataset:\n",
    "    case \"MNIST\":\n",
    "        dataset_cls = MNIST\n",
    "        input_shape = (28, 28, 1)\n",
    "        num_classes = 10\n",
    "    case \"EMNIST\":\n",
    "        dataset_cls = partial(EMNIST, split=\"bymerge\")\n",
    "        input_shape = (28, 28, 1)\n",
    "        num_classes = 42\n",
    "    case \"FashionMNIST\":\n",
    "        dataset_cls = FashionMNIST\n",
    "        input_shape = (28, 28, 1)\n",
    "        num_classes = 10\n",
    "    case _:\n",
    "        raise ValueError(f\"Unknown dataset: {dataset}.\")\n",
    "\n",
    "\n",
    "def load_data() -> tuple[jnp.ndarray, jnp.ndarray, jnp.ndarray, jnp.ndarray]:\n",
    "    \"\"\"\n",
    "    Load the training and test datasets.\n",
    "\n",
    "    Returns: Training set images, training set labels, test set images,\n",
    "        test set labels\n",
    "    \"\"\"\n",
    "    data_train = dataset_cls(data_dir, train=True, download=True)\n",
    "    data_test = dataset_cls(data_dir, train=False, download=True)\n",
    "\n",
    "    x_train = data_train.data.numpy()\n",
    "    y_train = data_train.targets.numpy()\n",
    "    x_test = data_test.data.numpy()\n",
    "    y_test = data_test.targets.numpy()\n",
    "\n",
    "    # Normalize the data\n",
    "    x_train = x_train / 255.0\n",
    "    x_test = x_test / 255.0\n",
    "\n",
    "    return x_train, y_train, x_test, y_test"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) IBP Initialization\n",
    "This code is based on https://github.com/jax-ml/jax/blob/5a2e5a5a94f78c96871a63a8a730164f1445d7a6/jax/_src/nn/initializers.py\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def _compute_fans(\n",
    "    shape: Sequence[int],\n",
    "    in_axis: int | Sequence[int] = -2,\n",
    "    out_axis: int | Sequence[int] = -1,\n",
    "    batch_axis: int | Sequence[int] = (),\n",
    ") -> tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Compute effective input and output sizes for a linear or convolutional layer.\n",
    "\n",
    "    Axes not in in_axis, out_axis, or batch_axis are assumed to constitute the\n",
    "    \"receptive field\" of a convolution (kernel spatial dimensions).\n",
    "    \"\"\"\n",
    "    if len(shape) <= 1:\n",
    "        raise ValueError(\n",
    "            f\"Can't compute input and output sizes of a {len(shape)}\"\n",
    "            \"-dimensional weights tensor. Must be at least 2D.\"\n",
    "        )\n",
    "\n",
    "    if isinstance(in_axis, int):\n",
    "        in_size = shape[in_axis]\n",
    "    else:\n",
    "        in_size = math.prod([shape[i] for i in in_axis])\n",
    "    if isinstance(out_axis, int):\n",
    "        out_size = shape[out_axis]\n",
    "    else:\n",
    "        out_size = math.prod([shape[i] for i in out_axis])\n",
    "    if isinstance(batch_axis, int):\n",
    "        batch_size = shape[batch_axis]\n",
    "    else:\n",
    "        batch_size = math.prod([shape[i] for i in batch_axis])\n",
    "    receptive_field_size = math.prod(shape) / in_size / out_size / batch_size\n",
    "    fan_in = in_size * receptive_field_size\n",
    "    fan_out = out_size * receptive_field_size\n",
    "    return fan_in, fan_out\n",
    "\n",
    "\n",
    "def ibp_init(\n",
    "    in_axis: int | Sequence[int] = -2,\n",
    "    out_axis: int | Sequence[int] = -1,\n",
    "    batch_axis: Sequence[int] = (),\n",
    ") -> Initializer:\n",
    "    \"\"\"\n",
    "    IBP-specific weight initialization as defined by Shi et al. (2021).\n",
    "\n",
    "    Args:\n",
    "      in_axis: axis or sequence of axes of the input dimension in the weights\n",
    "        array.\n",
    "      out_axis: axis or sequence of axes of the output dimension in the weights\n",
    "        array.\n",
    "      batch_axis: axis or sequence of axes in the weight array that should be\n",
    "        ignored.\n",
    "    \"\"\"\n",
    "\n",
    "    def init(key, shape, dtype=jnp.float32):\n",
    "        # shape = jax.core.canonicalize_shape(shape)\n",
    "        dtype = jax.dtypes.canonicalize_dtype(dtype)\n",
    "        fan_in, _ = _compute_fans(shape, in_axis, out_axis, batch_axis)\n",
    "        stddev = jnp.sqrt(2 * jnp.pi) / fan_in\n",
    "\n",
    "        if jnp.issubdtype(dtype, jnp.floating):\n",
    "            # constant is stddev of standard normal truncated to (-2, 2)\n",
    "            stddev = stddev / jnp.array(0.87962566103423978, dtype)\n",
    "            return jax.random.truncated_normal(key, -2, 2, shape, dtype) * stddev\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported dtype: {dtype}\")\n",
    "\n",
    "    return init"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### 3) Model definition "
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class CNN(nn.Module):\n",
    "    \"\"\"CNN model definition with the architecture from `net_arch`.\"\"\"\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, training=True):\n",
    "        x = x.reshape(-1, *input_shape)\n",
    "\n",
    "        prev_is_image = len(input_shape) > 1\n",
    "        for layer in model_arch:\n",
    "            match layer:\n",
    "                case (\"conv\", filters, kernel_size):\n",
    "                    # Omit bias, as batch norm would effectively remove it anyway\n",
    "                    x = nn.Conv(\n",
    "                        features=filters,\n",
    "                        kernel_size=(kernel_size, kernel_size),\n",
    "                        use_bias=False,\n",
    "                        kernel_init=ibp_init(),\n",
    "                    )(x)\n",
    "                    x = nn.BatchNorm(use_running_average=not training)(x)\n",
    "                case (\"relu\",):\n",
    "                    x = nn.relu(x)\n",
    "                case (\"avg_pool\", window_size, stride):\n",
    "                    x = nn.avg_pool(\n",
    "                        x,\n",
    "                        window_shape=(window_size, window_size),\n",
    "                        strides=(stride, stride),\n",
    "                    )\n",
    "                case (\"dense\", features):\n",
    "                    if prev_is_image:\n",
    "                        x = x.reshape(x.shape[0], -1)\n",
    "                        prev_is_image = False\n",
    "                    x = nn.Dense(\n",
    "                        features=features, use_bias=False, kernel_init=ibp_init()\n",
    "                    )(x)\n",
    "                    x = nn.BatchNorm(use_running_average=not training)(x)\n",
    "        return nn.Dense(features=num_classes)(x)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### 4. Loss Function\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def robustness_loss(model_fn, images, labels_one_hot, eps):\n",
    "    \"\"\"Uses IBP bounds to compute the guaranteed cross entropy loss.\"\"\"\n",
    "    # Input bounds for IBP\n",
    "    in_lb = jnp.clip(images - eps, 0.0, 1.0)  # Lower bound\n",
    "    in_ub = jnp.clip(images + eps, 0.0, 1.0)  # Upper bound\n",
    "\n",
    "    # Compute output bounds\n",
    "    # Do not update batch norm stats during IBP => training=False\n",
    "    out_lb, out_ub = ibp(model_fn)(Box(in_lb, in_ub), training=False)\n",
    "\n",
    "    # True class logits to lower bound\n",
    "    robust_scores = labels_one_hot * out_lb\n",
    "    # Other class logits to upper bound\n",
    "    robust_scores += (1.0 - labels_one_hot) * out_ub\n",
    "\n",
    "    log_probs = nn.log_softmax(robust_scores)\n",
    "    robust_loss = -jnp.mean(jnp.sum(labels_one_hot * log_probs, axis=-1))\n",
    "    return robust_loss, robust_scores\n",
    "\n",
    "\n",
    "def natural_loss(model_fn, images, labels_one_hot):\n",
    "    \"\"\"Computes the cross-entropy loss on the natural/clean data.\"\"\"\n",
    "    scores, variable_updates = model_fn(images, training=True, mutable=[\"batch_stats\"])\n",
    "    log_probs = nn.log_softmax(scores)\n",
    "    cross_entropy = -jnp.mean(labels_one_hot * log_probs)\n",
    "    return cross_entropy, (scores, variable_updates)\n",
    "\n",
    "\n",
    "def l2_loss(param):\n",
    "    \"\"\"L2 regularization\"\"\"\n",
    "    return jnp.mean(param**2)\n",
    "\n",
    "\n",
    "def loss_fn(state, batch, model_params, *, robustness_eps):\n",
    "    # Standard forward pass\n",
    "    images = batch[\"images\"]\n",
    "    labels_one_hot = jax.nn.one_hot(batch[\"labels\"], num_classes)\n",
    "\n",
    "    model_fn = partial(\n",
    "        state.apply_fn, {\"params\": model_params, \"batch_stats\": state.batch_stats}\n",
    "    )\n",
    "\n",
    "    nat_loss, (scores, updates) = natural_loss(model_fn, images, labels_one_hot)\n",
    "    l2_reg = sum(l2_loss(w) for w in jax.tree.leaves(model_params)) / len(model_params)\n",
    "\n",
    "    # we may set robustness_eps to 0 during warmup\n",
    "    if robustness_eps > 0:\n",
    "        rob_loss, robust_scores = robustness_loss(\n",
    "            model_fn, images, labels_one_hot, robustness_eps\n",
    "        )\n",
    "        robust_acc = jnp.mean(jnp.argmax(robust_scores, axis=-1) == batch[\"labels\"])\n",
    "    else:\n",
    "        rob_loss = robust_acc = 0.0\n",
    "\n",
    "    loss = (1 - robustness_loss_weight) * (\n",
    "        nat_loss + weight_decay * l2_reg\n",
    "    ) + robustness_loss_weight * rob_loss\n",
    "    nat_acc = jnp.mean(jnp.argmax(scores, axis=-1) == batch[\"labels\"])\n",
    "    return loss, (nat_loss, rob_loss, nat_acc, robust_acc, updates)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### 5) Evaluation Functions"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "@jax.jit\n",
    "def eval_step(state, batch):\n",
    "    scores = state.apply_fn(\n",
    "        {\"params\": state.params, \"batch_stats\": state.batch_stats},\n",
    "        batch[\"images\"],\n",
    "        training=False,\n",
    "    )\n",
    "    return scores\n",
    "\n",
    "\n",
    "def accuracy(state, batch):\n",
    "    preds = eval_step(state, batch)\n",
    "    preds = jnp.argmax(preds, axis=-1)\n",
    "    return jnp.mean(preds == batch[\"labels\"])\n",
    "\n",
    "\n",
    "# Certified accuracy computation using IBP bounds\n",
    "@jax.jit\n",
    "def certified_accuracy(state, batch, eps):\n",
    "    # Perturbation bounds\n",
    "    in_lb = jnp.clip(batch[\"images\"] - eps, 0.0, 1.0)  # Lower bound\n",
    "    in_ub = jnp.clip(batch[\"images\"] + eps, 0.0, 1.0)  # Upper bound\n",
    "\n",
    "    # Compute output bounds with IBP\n",
    "    out_lb, out_ub = ibp(state.apply_fn)(\n",
    "        {\"params\": state.params, \"batch_stats\": state.batch_stats},\n",
    "        Box(in_lb, in_ub),\n",
    "        training=False,\n",
    "    )\n",
    "\n",
    "    correct_class = batch[\"labels\"]\n",
    "\n",
    "    # Get the lower bound for the correct class and the upper bounds for all other classes\n",
    "    correct_class_lb = jnp.take_along_axis(\n",
    "        out_lb, correct_class[:, None], axis=-1\n",
    "    ).squeeze(axis=-1)\n",
    "    other_classes_ub = jnp.max(\n",
    "        jnp.where(\n",
    "            jnp.arange(out_lb.shape[-1]) == correct_class[:, None], -jnp.inf, out_ub\n",
    "        ),\n",
    "        axis=-1,\n",
    "    )\n",
    "\n",
    "    certifiably_correct = correct_class_lb > other_classes_ub\n",
    "    return jnp.mean(certifiably_correct)\n",
    "\n",
    "\n",
    "def full(eval_fn, state, images, labels, batch_size, **kwargs):\n",
    "    \"\"\"Evaluates a metric like accuracy on an entire dataset.\"\"\"\n",
    "    metric = 0.0\n",
    "    for i in range(0, len(images), batch_size):\n",
    "        batch = {\n",
    "            \"images\": images[i : i + batch_size],\n",
    "            \"labels\": labels[i : i + batch_size],\n",
    "        }\n",
    "        metric += eval_fn(state, batch, **kwargs) * len(batch[\"images\"])\n",
    "    return metric / len(images)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training step function:\n",
    "- Pass inputs through model \n",
    "- Compute loss (cross entropy using log_softmax outputs + l2_regularization + robust loss)\n",
    "- Compute gradients\n",
    "- Update params in training state via gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "rng_key = jax.random.PRNGKey(seed)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "x_train, y_train, x_test, y_test = load_data()\n",
    "x_train = x_train.reshape((-1, *input_shape))\n",
    "x_test = x_test.reshape((-1, *input_shape))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "model = CNN()\n",
    "rng_key, subkey = jax.random.split(rng_key)\n",
    "# Initialize both params and running batch statistics\n",
    "model_variables = model.init(subkey, jnp.ones((1, *input_shape)), training=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "optim = optax.adam(learning_rate)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class TrainState(flax.training.train_state.TrainState):\n",
    "    batch_stats: Any\n",
    "\n",
    "\n",
    "train_state = TrainState.create(apply_fn=model.apply, **model_variables, tx=optim)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "@partial(jax.jit, static_argnames=\"robustness_eps\")\n",
    "def train_step(state, batch, *, robustness_eps):\n",
    "    loss = partial(loss_fn, state, batch, robustness_eps=robustness_eps)\n",
    "    grad_fn = jax.value_and_grad(loss, has_aux=True)\n",
    "    (loss, (nat_loss, robust_loss, nat_acc, robust_acc, updates)), grads = grad_fn(\n",
    "        state.params\n",
    "    )\n",
    "    new_state = state.apply_gradients(grads=grads)\n",
    "    new_state = new_state.replace(batch_stats=updates[\"batch_stats\"])\n",
    "    return new_state, loss, nat_loss, robust_loss, nat_acc, robust_acc"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Running the training loop below for EMNIST takes approx. **80 min** on machine without GPU"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "prepare_epochs, warmup_epochs, _ = num_epochs\n",
    "total_epochs = sum(num_epochs)\n",
    "epoch_len = len(x_train) // batch_size\n",
    "\n",
    "log_epochs = epoch_len // 5\n",
    "\n",
    "for epoch in range(total_epochs):\n",
    "    if epoch < prepare_epochs:\n",
    "        eps_ = 0.0\n",
    "    elif epoch < prepare_epochs + warmup_epochs:\n",
    "        eps_ = eps * (epoch - prepare_epochs + 1) / (warmup_epochs + 1)\n",
    "    else:\n",
    "        eps_ = eps\n",
    "\n",
    "    running_loss = running_nat_loss = running_rob_loss = running_nat_acc = (\n",
    "        running_rob_acc\n",
    "    ) = 0.0\n",
    "\n",
    "    # Reshuffle the training set every epoch\n",
    "    rng_key, subkey = jax.random.split(rng_key)\n",
    "    train_idx = jax.random.permutation(subkey, len(x_train))\n",
    "    for i in range(epoch_len):\n",
    "        batch_images = x_train[train_idx[i * batch_size : (i + 1) * batch_size]]\n",
    "        batch_labels = y_train[train_idx[i * batch_size : (i + 1) * batch_size]]\n",
    "\n",
    "        batch = {\"images\": batch_images, \"labels\": batch_labels}\n",
    "        train_state, loss, nat_loss, rob_loss, nat_acc, rob_acc = train_step(\n",
    "            train_state, batch, robustness_eps=eps_\n",
    "        )\n",
    "\n",
    "        running_loss += loss * (1 / log_epochs)\n",
    "        running_nat_loss += nat_loss * (1 / log_epochs)\n",
    "        running_rob_loss += rob_loss * (1 / log_epochs)\n",
    "        running_nat_acc += nat_acc * (1 / log_epochs)\n",
    "        running_rob_acc += rob_acc * (1 / log_epochs)\n",
    "\n",
    "        if i % log_epochs == log_epochs - 1:\n",
    "            test_nat_acc = full(accuracy, train_state, x_test, y_test, eval_batch_size)\n",
    "            test_rob_acc = full(\n",
    "                certified_accuracy,\n",
    "                train_state,\n",
    "                x_test,\n",
    "                y_test,\n",
    "                eval_batch_size,\n",
    "                eps=eps,\n",
    "            )\n",
    "            print(\n",
    "                f\"[Epoch {epoch + 1}, {100 * i / epoch_len:3.0f}% (eps={eps_:.4f})] \"\n",
    "                f\"Loss: {running_loss:.6f}, \"\n",
    "                f\"Nat. Loss: {running_nat_loss:.6f}, \"\n",
    "                f\"Rob. Loss: {running_rob_loss:.6f}, \"\n",
    "                f\"Train Nat. Accuracy: {running_nat_acc * 100:.2f}%, \"\n",
    "                f\"Train Rob. Accuracy: {running_rob_acc * 100:.2f}%, \"\n",
    "                f\"Test Nat. Accuracy: {test_nat_acc * 100:.2f}%, \"\n",
    "                f\"Test Rob. Accuracy: {test_rob_acc * 100:.2f}%\"\n",
    "            )\n",
    "            running_loss = running_nat_loss = running_rob_loss = running_nat_acc = (\n",
    "                running_rob_acc\n",
    "            ) = 0.0"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "train_acc = full(accuracy, train_state, x_train, y_train, eval_batch_size)\n",
    "train_cert_acc = full(\n",
    "    certified_accuracy, train_state, x_train, y_train, eval_batch_size, eps=eps\n",
    ")\n",
    "test_acc = full(accuracy, train_state, x_test, y_test, eval_batch_size)\n",
    "test_cert_acc = full(\n",
    "    certified_accuracy, train_state, x_test, y_test, eval_batch_size, eps=eps\n",
    ")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\n",
    "    f\"Training Set: \"\n",
    "    f\"Natural Accuracy {train_acc * 100:.2f}%, \"\n",
    "    f\"Certified Accuracy {train_cert_acc * 100:.2f}%\"\n",
    ")\n",
    "print(\n",
    "    f\"Test Set:     \"\n",
    "    f\"Natural Accuracy {train_acc * 100:.2f}%, \"\n",
    "    f\"Certified Accuracy {train_cert_acc * 100:.2f}%\"\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Save the model parameters\n",
    "with open(f\"robust_{dataset.lower()}_cnn_flax.pkl\", \"wb\") as f:\n",
    "    pickle.dump(train_state.params, f)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
